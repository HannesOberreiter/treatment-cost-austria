## Multiple Regression

Starting with the base model including only the reported number of colonies wintered ($log_{10}$), the linear regression model did improve by adding the operational factors and as the last step the treatment methods \@ref(tab:model-table). The best model was with all predictors included, to evaluate eventual overfitting the predictive ability for out of sample and training set was evaluated, which shows that the training set (rmse = `r r_model$prediction_stats %>% filter(type == "Training") %>% pull(.estimate) %>% round(., 3)`) and testing set (rmse = `r r_model$prediction_stats %>% filter(type == "Test") %>% pull(.estimate) %>% round(., 3)`) have a similar root mean square error.


```{r model-table, include=T}
c <- "Summary of model fit. Base model only includes number of colonies wintered, operational the two questions migratory beekeeper and certified organic beekeeper. The eleven treatment methods are binary encoded, excluded is drone brood removal."
cn <- c("Model", "R^2", "adj. R^2", "df", "logLik", "AIC", "BIC")
tab <- kable(
    r_model$fitted %>%
        mutate(
            model = c("Intercept", "Base", "+ Operational", "+ Treatment Methods"),
            df = ifelse(is.na(df), 0, df)
        ) %>%
        select(model, r.squared, adj.r.squared, df:BIC),
    booktabs = TRUE,
    col.names = cn,
    digits = 3,
    align = c("l", rep("r", 9)),
    caption = c
)
tab
rm(cn, c, tab)
```
